<html>
    <head>
        <title>Classification Techniques</title>
        <style>
            h1 {
                border-bottom: 1px solid;
                font-size:38px;
                font-family:"segoe ui";
                font-weight: 700;
            }
            h2{

            }
            .div1{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div2{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div3{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div4{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div5{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div6{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div7{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            .div8{font-size:17.2px;
                   font-family: 'segoe ui';
            }
            #p1{
                font-size:23px;
                font-family:'segoe ui';
                font-weight: 650;
                text-align: center;
            }
            .center1 {
                display: block;
                margin-left: auto;
                margin-right: auto;
            }
            .center2 {
                display: block;
                margin-left: auto;
                margin-right: auto;
            }
        </style>   
    </head>

    <body bgcolor="#DCDCDC">
        <h1>CLASSIFICATION TECHNIQUES</h1>
        <div class="div1">
            <p>
                This section enlists the various classification techniques used to reach the final output.
                Till now, we have ranked features of the data using various feataure selction algorithms,
                and aggregated all the ranks to generate the final ranking so as to select a threshold value 
                offeatures and further use it in classification for better results. The idea of Classification Algorithms is pretty simple. 
                You predict the target class by analyzing the training dataset. 
            </p>
        </div>
        <div class="div2">
            <h2 style="border-bottom: 1px solid;"><span>Logistic Regression</span></h2>
            <p>Logistic regression is named for the function used at the core of the method, the logistic function</p>
            <p>The logistic function, also called the sigmoid function
                was developed by statisticians to describe properties of
                population growth in ecology, rising quickly and maxing
                out at the carrying capacity of the environment. It’s an
                S-shaped curve that can take any real-valued number and
                map it into a value between 0 and 1, but never exactly at
                those limits.
            </p>
            <img src="E:\Classifier/logistic.jpg" alt="logistic regression" class="center1">  
            
            <p>
                Where e is the base of the natural logarithms (Euler’s number or the EXP() function in your spreadsheet) and
                value is the actual numerical value that you want to transform. Logistic regression uses an equation as the representation,
                very much like linear regression. Input values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter
                Beta) to predict an output value (y). A key difference from linear regression is that the output value being
                modeled is a binary values (0 or 1) rather than a numeric value.
            </p>
            <p>Below is an example logistic regression equation:</p>
            <img src="E:\Classifier/logistic2.jpg" alt="logistic regression" class="center2">  
            <p>
                Where y is the predicted output, b0 is the bias or intercept term and b1 is the coefficient for the single input value (x). Each column in your input data has an associated b
                coefficient (a constant real value) that must be learned from your training data. The actual representation of the model that you would
                store in memory or in a file are the coefficients in the equation (the beta value or b’s).
            </p>
        </div>    
        <div class="div3">
            <h2 style="border-bottom: 1px solid;"><span>Support Vector Machine(SVM)</span></h2>
            <p>
                In machine learning, support-vector machines. are supervised
                learning models with associated learning algorithms that
                analyze data used for classification and regression analysis. Given a set of training examples, each marked
                as belonging to one or the other of two categories, an
                SVM training algorithm builds a model that assigns new
                examples to one category or the other, making it a nonprobabilistic binary linear classifier (although methods
                such as Platt scaling exist to use SVM in a probabilistic
                classification setting). An SVM model is a representation
                of the examples as points in space, mapped so that the
                examples of the separate categories are divided by a clear
                gap that is as wide as possible. New examples are then
                mapped into that same space and predicted to belong to
                a category based on the side of the gap on which they
                fall.
            </p>
            <p>
                More formally, a support-vector machine constructs a
                hyperplane or set of hyperplanes in a high- or infinitedimensional space, 
                which can be used for classification, regression, or other tasks like 
                outliers detection. Intuitively, a good separation is achieved by the
                hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional
                margin), since in general the larger the margin, the lower
                the generalization error of the classifier.
            </p>
            <p>
                To keep the computational load reasonable, the mappings
                used by SVM schemes are designed to ensure that dot
                products of pairs of input data vectors may be computed
                easily in terms of the variables in the original space,
                by defining them in terms of a kernel function k(x,y)
                selected to suit the problem. The hyperplanes in the
                higher-dimensional space are defined as the set of points
                whose dot product with a vector in that space is constant,
                where such a set of vectors is an orthogonal (and thus
                minimal) set of vectors that defines a hyperplane.
            </p>
        </div>
        <div class="div4">
            <h2 style="border-bottom: 1px solid;"><span>Decision Tree Classifier</span></h2>
            <p>
                A Decision Tree is a simple representation for classifying examples. It is a Supervised
                Machine Learning where the data is continuously split according to a certain parameter.
            </p>
            <p>Decision Tree consists of:</p>
            <ul>
                <li>Nodes : Test for the value of a certain attribute.</li>
                <li>Edges/ Branch : Correspond to the outcome of a test and connect to the next node or leaf.</li>
                <li>Leaf nodes : Terminal nodes that predict the outcome(represent class labels or class distribution).</li>    
            </ul>
        </div>
        <div class="div5">
            <h2 style="border-bottom: 1px solid;"><span>Random Forest</span></h2>
            <p>
                It is an ensemble tree-based learning algorithm.
                 The Random Forest Classifier is a set of decision trees from randomly selected subset of training set.
                  It aggregates the votes from different decision trees to decide the final class of the test object.
            </p>
        <p>
            It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.
        </p>
        <p>It is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.</p>
        </div>
        <div class="div6">
            <h2 style="border-bottom: 1px solid;"><span>Extra Tree Classifier</span></h2>
            <p>
                Extremely Randomized Trees Classifier(Extra Trees Classifier) is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a “forest” to output it’s classification result.
                 In concept, it is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest.
            </p>
            <p>
                Each Decision Tree in the Extra Trees Forest is constructed from the original training sample.
                 Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index).
                 This random sample of features leads to the creation of multiple de-correlated decision trees.
            </p>
            
        </div>
        <div class="div7">
            <h2 style="border-bottom: 1px solid;"><span>Bagging Classifier</span></h2>
            <p>
                A Bagging classifier is an ensemble
                meta-estimator that fits base classifiers each on random
                subsets of the original dataset and then aggregate their
                individual predictions (either by voting or by averaging)
                to form a final prediction. Such a meta-estimator can
                typically be used as a way to reduce the variance of a
                black-box estimator (e.g., a decision tree), by introducing
                randomization into its construction procedure and then
                making an ensemble out of it.
            </p>
            <p>
                This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn
                as random subsets of the samples, then this algorithm is
                known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When
                random subsets of the dataset are drawn as random
                subsets of the features, then the method is known as
                Random Subspaces . Finally, when base estimators are
                built on subsets of both samples and features, then the
                method is known as Random Patches.
            </p>
        </div>
        <div class="div8">
            <h2 style="border-bottom: 1px solid;"><span>Gaussian Naive Bayes</span></h2>
            <p>
                Naive bayes is a supervised learning algorithm for classification so the task is to find
                the class of observation (data point) given the values of
                features. Naive bayes classifier calculates the probability
                of a class given a set of feature values (i.e. <b>p(y<sub>i</sub> — x<sub>1</sub>,
                x<sub>2</sub> , . . . , x<sub>n</sub>)</b>). Input this into Bayes’ theorem.
            </p>
            <p>
                The conditional probability for a single feature given the
                class label (i.e. <b>p(x<sub>1</sub> — y<sub>i</sub>)</b> ) can be more easily estimated
                from the data. The algorithm needs to store probability
                distributions of features for each class independently.
                For example, if there are 5 classes and 10 features, 50
                different probability distributions need to be stored. The
                type of distributions depend on the characteristics of
                features:
            </p>
            <ul>
                <li>For binary features (<b>Y/N, True/False, 0/1</b>): Bernoulli distribution</li>
                <li>For discrete features (i.e. <b>word counts</b>): Multinomial distribution</li>
                <li>For continuous features: Gaussian (<b>Normal</b>) distribution</li>
            </ul>
        </div>
    </body>
</html>